---
date: "2021-01-01"
title: Characterizing and Displaying Multivariate Data
type: book
weight: 30
math: true
---

## Random Vector

### Matrix Presentation

Suppose the data set is generated by measuring $p$ variables on $n$ samples, then the data can be expressed as a $n×p$ data matrix $\textbf{Y}$:

$$\mathbf{Y}=\left(\begin{array}{ccccc}
y_{11} & \cdots & y_{1 j} & \cdots & y_{1 p} \\\\
\vdots & & \vdots & & \vdots \\\\
y_{i 1} & \cdots & y_{i j} & \cdots & y_{i p} \\\\
\vdots & & \vdots & & \vdots \\\\
y_{n 1} & \cdots & y_{n j} & \cdots & y_{n p}
\end{array}\right)=\left(\begin{array}{c}
\mathbf{y}_{1}^{\prime} \\\\
\vdots \\\\
\mathbf{y}_{i}^{\prime} \\\\
\vdots \\\\
\mathbf{y}_{n}^{\prime}
\end{array}\right)$$

where $\mathbf{y}\_{i}=\left(y_{i 1}, \ldots, y_{i p}\right)^{\prime}$ consists of the $i$th row of $\textbf{Y}$.

### Mean Vector

- Population mean vector

  For a random vector $\textbf{y}=(Y_1,..., Y_p)'$: $$E(\mathbf{y})=\left(E\left(Y_{1}\right), \ldots,      E\left(Y_{p}\right)\right)^{\prime}=\left(\mu_{1}, \ldots, \mu_{p}\right)^{\prime}=\mathbf{\mu}$$
  
- Sample mean vector

  For random sample {$\textbf{y}\_1,...,\textbf{y}\_n$} with $\textbf{y}\_i=(y\_{i1},...,y\_{ip})'$:
  
  $$\overline{\mathbf{y}}=\frac{1}{n} \sum_{i=1}^{n} \mathbf{y}_{i}=\left(\bar{y}_{1}, \ldots, \bar{y}_{p}\right)^{\prime}, \text { where } \bar{y}_{j}=\frac{1}{n} \sum_{i=1}^{n} y_{i j}$$
  Note that {$\textbf{y}_1,...,\textbf{y}_n$} is a matrix.

### Covariance Matrix

**Population covariance matrix**
  
  For a random vector $\textbf{y}=(Y\_1,..., Y\_p)'$, the $p×p$ population covariance matrix $\mathbf{\Sigma}$ is defined by:
  
  $$\boldsymbol{\Sigma}=\operatorname{COV}(\mathbf{y})=E\left[(\mathbf{y}-\boldsymbol{\mu})(\mathbf{y}-\boldsymbol{\mu})^{\prime}\right]=\left(\begin{array}{cccc}
\sigma_{11} & \sigma_{12} & \cdots & \sigma_{1 p} \\\\
\sigma_{21} & \sigma_{22} & \cdots & \sigma_{2 p} \\\\
\vdots & & & \\\\
& \vdots & & \vdots \\\\
\sigma_{p 1} & \sigma_{p 2} & \cdots & \sigma_{p p}
\end{array}\right)$$

  where $\sigma_{jk}$ is the population covariance between $Y_j$ and $Y_k$, and $\sigma{jj}=\sigma_{j}^2$ is the population variance of $Y_j$.
  
**Sample covariance matrix**

For a random sample {$\textbf{y}\_1,...,\textbf{y}\_n$} with $\textbf{y}\_i=(y\_{i1},...,y_{ip})$, the $p×p$ sample covariance matrix $\mathbf{S}$ is difined by:
  
$$\mathbf{S}=\frac{1}{n-1} \sum_{i=1}^{n}\left(\mathbf{y}_{i}-\overline{\mathbf{y}}\right)\left(\mathbf{y}_{i}-\overline{\mathbf{y}}\right)^{\prime}=\left(\begin{array}{cccc}
s_{11} & s_{12} & \cdots & s_{1 p} \\\\
s_{21} & s_{22} & \cdots & s_{2 p} \\\\
\vdots & & & \\\\
& \vdots & & \vdots \\\\
s_{p 1} & s_{p 2} & \cdots & s_{p p}
\end{array}\right)$$

where $s_{jk}$ is the sample covariance between the $j$th and $k$th variable of the vector, and $s{jj}=s_{j}^2$ is the sample variance of the $j$th variable.

**Remarks**

- An alternative way to compute $\mathbf{\Sigma}$, i.e.

$$\boldsymbol{\Sigma}=E\left(\mathbf{y} \mathbf{y}^{\prime}\right)-\boldsymbol{\mu} \boldsymbol{\mu}^{\prime}$$

- Alternative ways to compute $\mathbf{S}$:

  $$\mathbf{S}=\frac{1}{n-1}\left(\sum_{i=1}^{n} \mathbf{y}_{i} \mathbf{y}_{i}^{\prime}-n \overline{\mathbf{y}} \overline{\mathbf{y}}^{\prime}\right)=\frac{1}{n-1} \mathbf{Y}^{\prime}\left(\mathbf{I}-\frac{1}{n} \mathbf{J}\right) \mathbf{Y}$$
-  The covariance matrix of $\mathbf{\overline{y}}$ is $\operatorname{COV}(\overline{\mathbf{y}})=\mathbf{\Sigma} / n$

### Correlation Matrix

**Population correlation matrix**

$$
\mathbf{P}=\left(\rho_{j k}\right)=\left(\begin{array}{cccc}
1 & \rho_{12} & \cdots & \rho_{1 p} \\\\
\rho_{21} & 1 & \cdots & \rho_{2 p} \\\\
\vdots & & & \\\\
& \vdots & & \vdots \\\\
\rho_{p 1} & \rho_{p 2} & \cdots & 1
\end{array}\right)
$$

**Sample correlation matrix**

$$
\mathbf{R}=\left(r_{j k}\right)=\left(\begin{array}{cccc}
1 & r_{12} & \cdots & r_{1 p} \\\\
r_{21} & 1 & \cdots & r_{2 p} \\\\
\vdots & & & \\\\
& \vdots & & \vdots \\\\
r_{p 1} & r_{p 2} & \cdots & 1
\end{array}\right)
$$

**Remarks (important)**

The correlation matrix can be obtained from the cov matrix and vice versa. Take sample level as an example:

$$
\mathbf{R}=\mathbf{D}\_{s}^{-1} \mathbf{S} \mathbf{D}\_{s}^{-1}, \text {and } \mathbf{S}=\mathbf{D}\_{s} \mathbf{R} \mathbf{D}\_{s}
$$

where the $p×p$ diagonal matrix $\mathbf{D}_s$ is defined by

$$
\mathbf{D}\_{s}=\operatorname{diag}\left(\sqrt{s\_{11}}, \sqrt{s\_{22}}, \ldots, \sqrt{s\_{p p}}\right)=\operatorname{diag}\left(s_{1}, s_{2}, \ldots, s_{p}\right)
$$
Use R to to verify $\mathbf{S}=\mathbf{D}_{s} \mathbf{R} \mathbf{D}_{s}$:

``` r
# Compute the cov and corr matrix
S <- round(cov(iris[, 1:4]), 2) # keep 2 decimals of the cov matrix
S
```

    ##              Sepal.Length Sepal.Width Petal.Length Petal.Width
    ## Sepal.Length         0.69       -0.04         1.27        0.52
    ## Sepal.Width         -0.04        0.19        -0.33       -0.12
    ## Petal.Length         1.27       -0.33         3.12        1.30
    ## Petal.Width          0.52       -0.12         1.30        0.58

``` r
R <- round(cor(iris[, 1:4]), 2)
R
```

    ##              Sepal.Length Sepal.Width Petal.Length Petal.Width
    ## Sepal.Length         1.00       -0.12         0.87        0.82
    ## Sepal.Width         -0.12        1.00        -0.43       -0.37
    ## Petal.Length         0.87       -0.43         1.00        0.96
    ## Petal.Width          0.82       -0.37         0.96        1.00

``` r
Ds <- diag(sqrt(diag(S))) # obtain the diagnoal matrix
round(Ds %*% R %*% Ds, 2)
```

    ##       [,1]  [,2]  [,3]  [,4]
    ## [1,]  0.69 -0.04  1.28  0.52
    ## [2,] -0.04  0.19 -0.33 -0.12
    ## [3,]  1.28 -0.33  3.12  1.29
    ## [4,]  0.52 -0.12  1.29  0.58

``` r
S
```

    ##              Sepal.Length Sepal.Width Petal.Length Petal.Width
    ## Sepal.Length         0.69       -0.04         1.27        0.52
    ## Sepal.Width         -0.04        0.19        -0.33       -0.12
    ## Petal.Length         1.27       -0.33         3.12        1.30
    ## Petal.Width          0.52       -0.12         1.30        0.58

## Usage of Sample Cov Matrix

### Measures of Overall Variablility

The sample cov matrix $\mathbf{S}$ is a multifaceted picture of the overall variation in the data. But sometimes it is desirable to have a single numerical value for the overall multivariate scatter.

- Generalized sample variance: the determinant $|\mathbf{S}|$
- Total sample variance: $tr(\mathbf{S})=\sum_{j=1}^{p}{s_{jj}}$

Remarks:

- An extremely small value of $|\mathbf{S}|$ may indicate either small scatter or multicollinearity.
  
  In 2 variables cases, $|\mathbf{S}|=s_{1}^{2} s_{2}^{2} (1-\rho^{2})$, extremely small $|\mathbf{S}|$ indicates $\rho$ is close to 1 or -1.

### Statistical Distance

There are two types of measurment for distance betwwen two p-variate vectors, $\mathbf{y}\_{1}=\left(y\_{11}, \ldots, y_{1 p}\right)^{\prime}$ and $\mathbf{y}_{2}=\left(y_{21}, \ldots, y_{2 p}\right)^{\prime}$.

- Euclidean distance/$L_2$ norm:

  $$\left\|\mathbf{y}\_{1}-\mathbf{y}\_{2}\right\|=\sqrt{\left(\mathbf{y}\_{1}-\mathbf{y}\_{2}\right)^{\prime}\left(\mathbf{y}\_{1}-\mathbf{y}\_{2}\right)}=\sqrt{\sum_{j=1}^{p}\left(y_{1 j}-y_{2 j}\right)^{2}}$$
  
  It does not consider the difference in variation of the variables and the correlations between the variables.
  
- Statistical/Mahalanobis distance:

  $$
d=\sqrt{\left(\mathbf{y}\_{1}-\mathbf{y}\_{2}\right)^{\prime} \mathbf{S}^{-1}\left(\mathbf{y}\_{1}-\mathbf{y}\_{2}\right)}$$

  Note: 
  - When we talk about "distance", it is about 2 rows.
  - When 2 variables are highly correlated, the information they contain are highly overlapped, so they are less important.
  - The statistical distance is indeed the Euclidean distance between the "transformed" vectors $\mathbf{S}^{-1 / 2} \mathbf{y}\_{1}$ and $\mathbf{S}^{-1 / 2} \mathbf{y}\_{2}$

``` r
# pairwise Euclidean distance of the first 6 rows
L2 <- dist(iris[1:6, 1:4])
L2
```

    ##           1         2         3         4         5
    ## 2 0.5385165                                        
    ## 3 0.5099020 0.3000000                              
    ## 4 0.6480741 0.3316625 0.2449490                    
    ## 5 0.1414214 0.6082763 0.5099020 0.6480741          
    ## 6 0.6164414 1.0908712 1.0862780 1.1661904 0.6164414



``` r
library(expm)
# pairwise statistical distance of the first 6 rows
S.inv.sqrt <- sqrtm(solve(S)) # obtain S^{-1/2]}
Y.tran <- as.matrix(iris[1:6, 1:4]) %*% S.inv.sqrt
d <- dist(Y.tran) # Euclidean distance of the transformed data matrix
d
```

    ##           1         2         3         4         5
    ## 2 1.3684919                                        
    ## 3 0.9719474 0.9526712                              
    ## 4 1.3617314 1.4096962 0.7028559                    
    ## 5 0.5777318 1.8208785 1.1476425 1.3145933          
    ## 6 1.1387877 2.4601984 1.9251824 2.2114699 0.9306823

### Visulization

``` r
pairs(iris[, 1:4], main="Scatterplot Matrix Iris Data")
```

![6ZRzLQ.jpg](https://s3.ax1x.com/2021/03/04/6ZRzLQ.jpg)


## Partitions of Random Vector

``` r
# cov matrix of y
S
```

    ##              Sepal.Length Sepal.Width Petal.Length Petal.Width
    ## Sepal.Length         0.69       -0.04         1.27        0.52
    ## Sepal.Width         -0.04        0.19        -0.33       -0.12
    ## Petal.Length         1.27       -0.33         3.12        1.30
    ## Petal.Width          0.52       -0.12         1.30        0.58

``` r
# one block cov matrix of y
round(cov((iris[, 1:2])), 2)
```

    ##              Sepal.Length Sepal.Width
    ## Sepal.Length         0.69       -0.04
    ## Sepal.Width         -0.04        0.19


## Linear Cominations

### Population linear combinations

Consider a p-variate random vector $\mathbf{y}=\left(Y_{1}, \ldots, Y_{p}\right)^{\prime}$ with mean $\mathbf{\mu}$ and cov matrix $\mathbf{\Sigma}$. Note that $\mathbf{\mu}$ is a p-dim vector.

Define the linear combination of $Y_1,...,Y_p$ as $Z = \mathbf{a}'\mathbf{y}=\sum_{j=1}^{p}Y_j$, where $\mathbf{a}=(a_1,...,a_p)'$ is the coefficient vector. Then the random variable $Z$ (a scalar rather than a vector) has:

$$E(Z)=E\left(\mathbf{a}^{\prime} \mathbf{y}\right)=\mathbf{a}^{\prime} \boldsymbol{\mu}, \quad \operatorname{var}(Z)=\operatorname{var}\left(\mathbf{a}^{\prime} \mathbf{y}\right)=\mathbf{a}^{\prime} \mathbf{\Sigma} \mathbf{a}$$



A more general linear transformation is $\mathbf{w}=\mathbf{A} \mathbf{y}+\mathbf{b}$. Then:

$$\mu_{\mathrm{w}}=E(\mathbf{A y}+\mathbf{b})=\mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \quad \boldsymbol{\Sigma}_{\mathbf{w}}=\operatorname{COV}(\mathbf{w})=\mathbf{A} \Sigma \mathbf{A}^{\prime}$$
where $\mathbf{A}$ is a $q×p$ matrix, indicating $q$ linear combinations among $p$ variables.

### Sample linear combinations

Consider random sample $z\_1,...,z\_n$ where $z\_i=\mathbf{a}'\mathbf{y}\_i$ which is a scalar.

$$
\bar{z}=\frac{1}{n} \sum\_{i=1}^{n}z\_{i}=\frac{1}{n} \sum\_{i=1}^{n}{\mathbf{a}'\mathbf{y}\_i}=\frac{1}{n} \mathbf{a}' \sum\_{i=1}^{n}{\mathbf{y\_{i}}}=\mathbf{a}' \bar{\mathbf{y}}
$$

$$
\begin{aligned}
s\_{z}^2 &=\frac{1}{n-1}\sum\_{i=1}^{n}{(z\_i-\bar{z})^2} \\\\ &=\frac{1}{n-1}\sum\_{i=1}^{n}{(\mathbf{a}'\mathbf{y}\_i-\mathbf{a}' \bar{\mathbf{y}})^2} \\\\ &=\frac{1}{n-1}\mathbf{a}'   (\sum\_{i=1}^{n}{(\mathbf{y}\_i-\bar{\mathbf{y}})^2} )\mathbf{a} \\\\ &=\mathbf{a}'s\_{y}^{2}\mathbf{a}
\end{aligned}
$$


Consider random sample $\mathbf{z}\_1,...,\mathbf{z}\_n$ where $\mathbf{z}\_i=\mathbf{A}\mathbf{y}\_i$ and $\mathbf{A}$ is a $q×p$ matrix.

$$
\bar{\mathbf{z}}=\frac{1}{n} \sum\_{i=1}^{n}\mathbf{z}\_{i}=\frac{1}{n} \sum\_{i=1}^{n}{\mathbf{A}\mathbf{y}\_i}=\frac{1}{n} \mathbf{A} \sum\_{i=1}^{n}{\mathbf{y\_{i}}}=\mathbf{A} \bar{\mathbf{y}}
$$

$$
\begin{aligned}
\mathbf{S}\_{z}&=\frac{1}{n-1} \sum\_{n=1}^{n}\left(\mathbf{z}\_{i}-\overline{\mathbf{z}}\right)\left(\mathbf{z}\_{i}-\overline{\mathbf{z}}\right)^{\prime} \\\\
&=\frac{1}{n-1} \sum\_{n=1}^{n}\left(\mathbf{A} \mathbf{y}\_{i}-\mathbf{A} \overline{\mathbf{y}}\right)\left(\mathbf{A} \mathbf{y}\_{i}-\mathbf{A} \overline{\mathbf{y}}\right)^{\prime} \\\\
&=\frac{1}{n-1} \mathbf{A}\left(\sum\_{n=1}^{n}\left(\mathbf{y}\_{i}-\overline{\mathbf{y}}\right)\left(\mathbf{y}\_{i}-\overline{\mathbf{y}}\right)^{\prime}\right) \mathbf{A}^{\prime} \\\\
&=\mathbf{A} \mathbf{S}\_{y} \mathbf{A}^{\prime}
\end{aligned}
$$


